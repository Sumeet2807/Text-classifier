{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "class Text_reducer():\n",
    "    def __init__(self,core_words=[]):\n",
    "        self.core_words = core_words\n",
    "    def __call__(self,s): \n",
    "        red_text = ''   \n",
    "        if len(self.core_words):\n",
    "            match_pattern = r'(' + re.escape(self.core_words[0])\n",
    "            for i in range(1,len(self.core_words)):\n",
    "                match_pattern += (r'|' + re.escape(self.core_words[i]))\n",
    "            match_pattern += r')'\n",
    "            sents = re.split(';|\\.', s)\n",
    "            for sent in sents:\n",
    "                if re.findall(match_pattern, sent):            \n",
    "                    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "                    sent = re.sub('\\d*\\.?\\d+', ' num ', sent)\n",
    "                    red_text = '\\n'.join([red_text, sent])\n",
    "        else:\n",
    "            red_text = re.sub('[^A-Za-z0-9]+', ' ', s)\n",
    "            red_text = re.sub('\\d*\\.?\\d+', ' num ', s)\n",
    "            \n",
    "        return red_text\n",
    "\n",
    "class Text_preprocessor():\n",
    "    def __init__(self,force_lower_case=False,reduce_text=None,remove_stop_words=True, lemmatize=True):\n",
    "        self.reduce_text = reduce_text\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.lemmatize = lemmatize\n",
    "        self.force_lower_case = force_lower_case\n",
    "    def __call__(self,x):\n",
    "        x = str(x)\n",
    "        if self.force_lower_case:\n",
    "            x = x.lower()\n",
    "        if self.reduce_text:\n",
    "            x = self.reduce_text(x)\n",
    "        if self.remove_stop_words:        \n",
    "            stop_words = set(stopwords.words('english'))\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        if self.lemmatize or self.remove_stop_words:\n",
    "            word_tokens = word_tokenize(str(x))\n",
    "\n",
    "            if self.lemmatize and self.remove_stop_words:\n",
    "                filtered_sentence = [lemmatizer.lemmatize(w) for w in word_tokens if not w.lower() in stop_words]\n",
    "            elif self.lemmatize:\n",
    "                filtered_sentence = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "            elif self.remove_stop_words:\n",
    "                filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "\n",
    "            x = ' '.join(filtered_sentence).lower()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "reducer = Text_reducer(['rebate'])\n",
    "preprocessor = Text_preprocessor(True,reducer,False,False)\n",
    "preprocessor('ddf REBATE d df. fadf af saf and')\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def csv_loader(file, text_column, category_column, sep=',',text_preprocessing=None, shuffle=False):\n",
    "    df = pd.read_csv(file, sep=sep)\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1)\n",
    "    X = df[text_column]\n",
    "    if text_preprocessing:\n",
    "        X = X.apply(text_preprocessing)\n",
    "    y = df[category_column]\n",
    "\n",
    "    return X.to_numpy(), y.to_numpy()\n",
    "\n",
    "file = '../data/predictions_greater_than_2021-1-1_v2.csv'\n",
    "text_column = 'Actual text'\n",
    "category_column = 'Predictions'\n",
    "X,y = csv_loader(file, text_column, category_column,text_preprocessing=preprocessor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.preprocessing import Text_reducer\n",
    "from utils.preprocessing import Text_preprocessor\n",
    "\n",
    "def csv_loader(file, text_column, category_column, sep=',',text_preprocessing=None, shuffle=False):\n",
    "    df = pd.read_csv(file, sep=sep)\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1)\n",
    "    X = df[text_column]\n",
    "    if text_preprocessing:\n",
    "        X = X.apply(text_preprocessing)\n",
    "    y = df[category_column]\n",
    "\n",
    "    return X.to_numpy(), y.to_numpy()\n",
    "\n",
    "reducer = Text_reducer(['rebate'])\n",
    "preprocessor = Text_preprocessor(True,reducer,False,False)\n",
    "\n",
    "\n",
    "file = ''\n",
    "text_column = 'Actual text'\n",
    "category_column = 'Predictions'\n",
    "X,y = csv_loader(file, text_column, category_column,text_preprocessing=preprocessor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4e011bea087348f089ed99a7f4744fa8e15b41a6914d8f39c20b970ed3434b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
